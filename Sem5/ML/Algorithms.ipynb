{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  7 30]\n",
      " [ 9  4  6  1]\n",
      " [ 8 15  2 40]\n",
      " [20 10  2  6]]\n",
      "[[0.         0.         1.         0.74358974]\n",
      " [0.38888889 0.08333333 0.8        0.        ]\n",
      " [0.33333333 1.         0.         1.        ]\n",
      " [1.         0.58333333 0.         0.12820513]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#defining an array\n",
    "df = np.array([[2,3,7,30],[9,4,6,1],[8,15,2,40],[20,10,2,6]])\n",
    "print (df)\n",
    "\n",
    "#normalizing the array\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df)\n",
    "scaled_features = scaler.transform(df)\n",
    "print(scaled_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  7 30]\n",
      " [ 9  4  6  1]\n",
      " [ 8 15  2 40]\n",
      " [20 10  2  6]]\n",
      "[[-1.19319056 -1.03142125  1.20740686  0.66200869]\n",
      " [-0.11547005 -0.825137    0.76834982 -1.12387522]\n",
      " [-0.26943013  1.44398974 -0.98787834  1.27783073]\n",
      " [ 1.57809074  0.4125685  -0.98787834 -0.8159642 ]]\n"
     ]
    }
   ],
   "source": [
    "#standard Scalar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "#defining an array\n",
    "df = np.array([[2,3,7,30],[9,4,6,1],[8,15,2,40],[20,10,2,6]])\n",
    "print(df)\n",
    "sc_X= StandardScaler()\n",
    "sc_X=sc_X.fit_transform(df)\n",
    "print(sc_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "iris = datasets.load_iris()\n",
    "data = pd.DataFrame(iris.get('data'),columns=['sepal length','petal length','sepal width','petal width'])\n",
    "data.head()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(data.iloc[:,:-1],data['sepal width'], test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Virginica' 'Versicolor' 'Setosa' 'Virginica' 'Versicolor' 'Versicolor'\n",
      " 'Setosa' 'Versicolor' 'Versicolor' 'Versicolor' 'Versicolor' 'Virginica'\n",
      " 'Virginica' 'Setosa' 'Versicolor' 'Setosa' 'Versicolor' 'Versicolor'\n",
      " 'Versicolor' 'Virginica' 'Virginica' 'Setosa' 'Setosa' 'Versicolor'\n",
      " 'Setosa' 'Virginica' 'Setosa' 'Virginica' 'Versicolor' 'Versicolor'\n",
      " 'Versicolor' 'Setosa' 'Setosa' 'Versicolor' 'Versicolor' 'Versicolor'\n",
      " 'Setosa' 'Virginica' 'Setosa' 'Versicolor' 'Versicolor' 'Virginica'\n",
      " 'Versicolor' 'Virginica' 'Virginica' 'Setosa' 'Virginica' 'Setosa'\n",
      " 'Versicolor' 'Virginica' 'Virginica' 'Virginica' 'Versicolor']\n",
      "[[14  0  0]\n",
      " [ 0 21  1]\n",
      " [ 0  2 15]]\n",
      "0.9433962264150944\n",
      "Probabilities of falling into each class\n",
      "[[0.00000000e+00 2.17832999e-33 1.00000000e+00]]\n",
      "input is classified into: \n",
      "['Virginica']\n"
     ]
    }
   ],
   "source": [
    "from distutils import text_file\n",
    "from inspect import getmodule\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"iris.csv\")\n",
    "x=dataset[['sepal.length','petal.length','sepal.width','petal.width']]\n",
    "y=dataset['variety']\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.35)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gmodel=GaussianNB()\n",
    "gmodel.fit(x_train,y_train)\n",
    "gpred=gmodel.predict(x_test)\n",
    "print(gpred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "gcm = confusion_matrix(y_test,gpred)\n",
    "print(gcm)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test,gpred)\n",
    "print (accuracy)\n",
    "\n",
    "print(\"Probabilities of falling into each class\")\n",
    "print(gmodel.predict_proba([[5,3,1.2,2]]))\n",
    "print(\"input is classified into: \")\n",
    "ans = gmodel.predict([[5,3,1.2,2]])\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d08b39f4dbef70f0d8175b93e3eb1909708a293dfc021300289f58bfc754009"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
